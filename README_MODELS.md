# Использование различных языковых моделей в RAG-системе

В этой RAG-системе вы можете использовать различные языковые модели для анализа криптовалютных новостей. Система поддерживает две модели, каждая со своими преимуществами и особенностями.

## Доступные модели

1. **Phi-3 Mini (Microsoft)** - `phi`
   - Легкая и быстрая модель от Microsoft
   - 3.8B параметров
   - Хорошо подходит для простых запросов
   - Работает на CPU с ограниченными ресурсами

2. **Qwen 7B (Alibaba)** - `qwen`
   - Мощная многоязычная модель
   - 7B параметров
   - Хорошо обрабатывает сложные запросы
   - Отличные аналитические способности

## Запуск с выбором модели

Для запуска системы с выбором модели используйте скрипт `run_rag.py`:

```bash
python run_rag.py --model phi --mode qa --query "Какие тенденции в Ethereum?"
```

### Параметры запуска:

- `--model` или `-m`: Выбор модели (`phi`, `qwen`)
- `--query` или `-q`: Запрос для анализа
- `--mode`: Режим анализа (`qa` - вопрос-ответ, `compare` - сравнение мнений)
- `--temperature` или `-t`: Температура генерации (0.0-1.0)
- `--list-models` или `-l`: Показать список доступных моделей
- `--index-path`: Путь к индексу векторной базы данных

## Примеры использования

### Показать список доступных моделей
```bash
python run_rag.py --list-models
```

### Ответить на вопрос с помощью Phi-3
```bash
python run_rag.py --model phi --query "Как блокчейн Ethereum влияет на DeFi?"
```

### Сравнить мнения с помощью Qwen
```bash
python run_rag.py --model qwen --mode compare --query "Перспективы биткоина"
```

### Интерактивный ввод запроса с Qwen
```bash
python run_rag.py --model qwen --mode qa
```

## Системные требования

Разные модели требуют разное количество ресурсов:

1. **Phi-3 Mini**: минимум 4ГБ VRAM или 8ГБ RAM для CPU
2. **Qwen 7B**: минимум 8ГБ VRAM или 16ГБ RAM для CPU

## Выбор модели для разных задач

- Для быстрых ответов и слабого оборудования: `phi`
- Для более глубокого анализа и сложных запросов: `qwen`

## Установка дополнительных пакетов

Для корректной работы с этими моделями требуются следующие библиотеки:

```bash
pip install transformers accelerate safetensors bitsandbytes huggingface_hub typing_extensions
```

## Устранение неполадок

Если возникают проблемы с памятью:
- Уменьшите параметр `max_new_tokens` в `llm_factory.py`
- Используйте модель с меньшим количеством параметров (Phi-3)
- Включите 4-битную квантизацию (если используете GPU) 